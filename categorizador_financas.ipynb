{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importando bibliotecas de análise inicial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puxando a base de dados\n",
    "df = pd.read_excel('dados_excel.xlsx')\n",
    "categorize_label = lambda x: x.astype('category')\n",
    "df[['Subcategoria']] = df[['Subcategoria']].apply(categorize_label)\n",
    "valor = df['Montante']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando as bibliotecas para normalização do texto\n",
    "from unicodedata import normalize\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Daniel\n",
      "[nltk_data]     Beigelman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Fazendo download de palavras que queremos retirar\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando lista com palavras indesejadas\n",
    "stop = stopwords.words(\"portuguese\")\n",
    "lista_stop = [normalize('NFKD', x).encode('ASCII', 'ignore').decode('ASCII') for x in stop] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['brasilia','sao paulo','br','osasco','lago norte','lago nort']:\n",
    "    lista_stop.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_text(raw_review):\n",
    "    # Função limpa e normaliza o texto para aprendizagem\n",
    "    letters_only=re.sub(\"[^a-zA-Z]\",\" \",normalize('NFKD', raw_review).encode('ASCII', 'ignore').decode('ASCII'))\n",
    "    words = letters_only.lower().split()\n",
    "    meaningful_words=[w for w in words if not w in lista_stop]\n",
    "    return(' '.join(meaningful_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o o tamanho do vetor\n",
    "num_desc = df['Nota'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria vetor com todas as descrições\n",
    "clean_desc=[]\n",
    "for i in range(num_desc):\n",
    "    clean_desc.append(correct_text(df['Nota'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split dos dados em treino e validação\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_desc, df[\"Subcategoria\"], test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando todas as bibliotecas para tratamento e treinamento dos dados\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_data = FunctionTransformer(lambda x: x['desc'].values, validate=False)\n",
    "get_numeric_data = FunctionTransformer(lambda x: x['valor'].values, validate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_...   warm_start=False))],\n",
       "         flatten_transform=None, n_jobs=None, voting='soft', weights=None))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgr = OneVsRestClassifier(LogisticRegression(random_state=3, multi_class=\"multinomial\",solver=\"lbfgs\", C=10))\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100,max_leaf_nodes=10, n_jobs=-1)\n",
    "\n",
    "pl = Pipeline([\n",
    "        ('vec', CountVectorizer(analyzer='word',ngram_range=(1,1), stop_words = None)),\n",
    "        ('voting_clf', VotingClassifier(estimators=[('lr', lgr), ('rf', rnd_clf)],voting='soft'))\n",
    "    ])\n",
    "pl.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9176470588235294"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = pl.score(X_test, y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementando com descrição e valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArrayCaster(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data):\n",
    "        print(data.shape)\n",
    "        print(np.transpose(np.matrix(data)).shape)\n",
    "        return np.transpose(np.matrix(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Juntando os dados e embaralhando\n",
    "clean_desc_2 = pd.DataFrame(clean_desc)\n",
    "df_cor = pd.concat([clean_desc_2,valor,df[\"Subcategoria\"]],axis=1)\n",
    "df_cor.columns = ['desc','valor','subcat']\n",
    "df_cor = df_cor.sample(frac=1,random_state=42).reset_index(drop=True)\n",
    "\n",
    "X = df_cor[['desc','valor']]\n",
    "y = df_cor['subcat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split dos dados em treino e validação\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl2 = Pipeline([\n",
    "#         ('union', FeatureUnion(\n",
    "#             transformer_list = [\n",
    "#                 ('text_fetuares', Pipeline([\n",
    "#                     ('selector', get_text_data),\n",
    "#                     ('vectorizer', CountVectorizer(analyzer='word',ngram_range=(1,1), stop_words = None))\n",
    "#                 ])),\n",
    "#                     ('numeric_features', Pipeline([\n",
    "#                     ('selector', get_numeric_data),\n",
    "#                     ('caster', ArrayCaster())\n",
    "#                 ]))\n",
    "#              ]\n",
    "#         )),\n",
    "#         ('voting_clf', VotingClassifier(estimators=[('lr', lgr), ('rf', rnd_clf)],voting='soft'))\n",
    "#     ])\n",
    "# pl.fit(X_train_2, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy = pl2.score(X_test_2, y_test_2)\n",
    "# accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
